{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpD4SnJHN8vCRyhrra+WwM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VzkYBpS4y4Ty","executionInfo":{"status":"ok","timestamp":1681788499978,"user_tz":-480,"elapsed":19044,"user":{"displayName":"BMW x","userId":"13500298887834359697"}},"outputId":"c8c17612-ccb5-4e46-faf9-21031d2263f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","\n","sys.path.append('/content/drive/MyDrive/fbgan/fbgan')\n","sys.path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3CVKr_N56jS","executionInfo":{"status":"ok","timestamp":1681788499978,"user_tz":-480,"elapsed":4,"user":{"displayName":"BMW x","userId":"13500298887834359697"}},"outputId":"56328145-cd75-428c-9628-84a3f86ead21"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content',\n"," '/env/python',\n"," '/usr/lib/python39.zip',\n"," '/usr/lib/python3.9',\n"," '/usr/lib/python3.9/lib-dynload',\n"," '',\n"," '/usr/local/lib/python3.9/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.9/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/drive/MyDrive/fbgan/fbgan']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#    Copyright (C) 2018 Anvita Gupta\n","#\n","#    This program is free software: you can redistribute it and/or  modify\n","#    it under the terms of the GNU Affero General Public License, version 3,\n","#    as published by the Free Software Foundation.\n","#\n","#    This program is distributed in the hope that it will be useful,\n","#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n","#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n","#    GNU Affero General Public License for more details.\n","#\n","#    You should have received a copy of the GNU Affero General Public License\n","#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n","#\n","\n","import torch\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.autograd as autograd\n","from torch.autograd import Variable\n","\n","from sklearn.preprocessing import OneHotEncoder\n","import os, math, glob, argparse\n","from utils.torch_utils import *\n","from utils.utils import *\n","import matplotlib.pyplot as plt\n","import utils.language_helpers\n","plt.switch_backend('agg')\n","import numpy as np\n","from models import *\n","\n","class WGAN_LangGP():\n","    def __init__(self, batch_size=256, lr=0.0001, num_epochs=80, seq_len = 249, data_dir='/content/drive/MyDrive/fbgan/fbgan/data/a.fa', \\\n","        run_name='test', hidden=512, d_steps = 10):\n","        self.hidden = hidden\n","        self.batch_size = batch_size\n","        self.lr = lr\n","        self.n_epochs = num_epochs\n","        self.seq_len = seq_len\n","        self.d_steps = d_steps\n","        self.g_steps = 1\n","        self.lamda = 10 #lambda\n","        self.checkpoint_dir = '/content/drive/MyDrive/fbgan/fbgan/checkpoint/' + run_name + \"/\"\n","        self.sample_dir = '/content/drive/MyDrive/fbgan/fbgan/samples/' + run_name + \"/\"\n","        self.load_data(data_dir)\n","        if not os.path.exists(self.checkpoint_dir): os.makedirs(self.checkpoint_dir)\n","        if not os.path.exists(self.sample_dir): os.makedirs(self.sample_dir)\n","        self.use_cuda = True if torch.cuda.is_available() else False\n","        self.build_model()\n","\n","    def build_model(self):\n","        self.G = Generator_lang(len(self.charmap), self.seq_len, self.batch_size, self.hidden)\n","        self.D = Discriminator_lang(len(self.charmap), self.seq_len, self.batch_size, self.hidden)\n","        if self.use_cuda:\n","            self.G.cuda()\n","            self.D.cuda()\n","        print(self.G)\n","        print(self.D)\n","        self.G_optimizer = optim.Adam(self.G.parameters(), lr=self.lr, betas=(0.5, 0.9))\n","        self.D_optimizer = optim.Adam(self.D.parameters(), lr=self.lr, betas=(0.5, 0.9))\n","\n","    def load_data(self, datadir):\n","        max_examples = 1e6\n","        lines, self.charmap, self.inv_charmap = utils.language_helpers.load_dataset(\n","            max_length=self.seq_len,\n","            max_n_examples=max_examples,\n","            data_dir=datadir\n","        )\n","        self.data = lines\n","\n","    def save_model(self, epoch):\n","        torch.save(self.G.state_dict(), self.checkpoint_dir + \"G_weights_{}.pth\".format(epoch))\n","        torch.save(self.D.state_dict(), self.checkpoint_dir + \"D_weights_{}.pth\".format(epoch))\n","\n","    def load_model(self, directory = ''):\n","        '''\n","            Load model parameters from most recent epoch\n","        '''\n","        if len(directory) == 0:\n","            directory = self.checkpoint_dir\n","        list_G = glob.glob(directory + \"G*.pth\")\n","        list_D = glob.glob(directory + \"D*.pth\")\n","        if len(list_G) == 0:\n","            print(\"[*] Checkpoint not found! Starting from scratch.\")\n","            return 1 #file is not there\n","        G_file = max(list_G, key=os.path.getctime)\n","        D_file = max(list_D, key=os.path.getctime)\n","        epoch_found = int( (G_file.split('_')[-1]).split('.')[0])\n","        print(\"[*] Checkpoint {} found at {}!\".format(epoch_found, directory))\n","        self.G.load_state_dict(torch.load(G_file))\n","        self.D.load_state_dict(torch.load(D_file))\n","        return epoch_found\n","\n","    def calc_gradient_penalty(self, real_data, fake_data):\n","        alpha = torch.rand(self.batch_size, 1, 1)\n","        alpha = alpha.view(-1,1,1)\n","        alpha = alpha.expand_as(real_data)\n","        alpha = alpha.cuda() if self.use_cuda else alpha\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","        interpolates = interpolates.cuda() if self.use_cuda else interpolates\n","        interpolates = autograd.Variable(interpolates, requires_grad=True)\n","\n","        disc_interpolates = self.D(interpolates)\n","\n","        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda() \\\n","                                  if self.use_cuda else torch.ones(disc_interpolates.size()),\n","                                  create_graph=True, retain_graph=True)[0]\n","\n","        gradients = gradients.contiguous().view(self.batch_size, -1)\n","        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n","\n","        #gradient_penalty = ((gradients.norm(2, dim=1).norm(2,dim=1) - 1) ** 2).mean() * self.lamda\n","        return self.lamda * ((gradients_norm - 1) ** 2).mean()\n","\n","    def disc_train_iteration(self, real_data):\n","        self.D_optimizer.zero_grad()\n","\n","        fake_data = self.sample_generator(self.batch_size)\n","        d_fake_pred = self.D(fake_data)\n","        d_fake_err = d_fake_pred.mean()\n","        d_real_pred = self.D(real_data)\n","        d_real_err = d_real_pred.mean()\n","\n","        gradient_penalty = self.calc_gradient_penalty(real_data, fake_data)\n","\n","        d_err = d_fake_err - d_real_err + gradient_penalty\n","        d_err.backward()\n","        self.D_optimizer.step()\n","\n","        return d_fake_err.data, d_real_err.data, gradient_penalty.data\n","\n","    def sample_generator(self, num_sample):\n","        z_input = Variable(torch.randn(num_sample, 128))\n","        if self.use_cuda: z_input = z_input.cuda()\n","        generated_data = self.G(z_input)\n","        return generated_data\n","\n","    def gen_train_iteration(self):\n","        self.G.zero_grad()\n","        z_input = to_var(torch.randn(self.batch_size, 128))\n","        g_fake_data = self.G(z_input)\n","        dg_fake_pred = self.D(g_fake_data)\n","        g_err = -torch.mean(dg_fake_pred)\n","        g_err.backward()\n","        self.G_optimizer.step()\n","        return g_err\n","\n","    def train_model(self, load_dir):\n","        init_epoch = self.load_model(load_dir)\n","        total_iterations = 4000\n","        losses_f = open(self.checkpoint_dir + \"losses.txt\",'a+')\n","        d_fake_losses, d_real_losses, grad_penalties = [],[],[]\n","        G_losses, D_losses, W_dist = [],[],[]\n","\n","        table = np.arange(len(self.charmap)).reshape(-1, 1)\n","        one_hot = OneHotEncoder()\n","        one_hot.fit(table)\n","\n","        counter = 0\n","        for epoch in range(self.n_epochs):\n","            n_batches = int(len(self.data)/self.batch_size)\n","            msg = 'In epoch {},n_batches is {},but total_iterations is{}'.format(epoch,n_batches,total_iterations)\n","            print(msg)\n","            for idx in range(n_batches):\n","                _data = np.array(\n","                    [[self.charmap[c] for c in l] for l in self.data[idx*self.batch_size:(idx+1)*self.batch_size]],\n","                    dtype='int32'\n","                )\n","                data_one_hot = one_hot.transform(_data.reshape(-1, 1)).toarray().reshape(self.batch_size, -1, len(self.charmap))\n","                real_data = torch.Tensor(data_one_hot)\n","                real_data = to_var(real_data)\n","\n","                d_fake_err, d_real_err, gradient_penalty = self.disc_train_iteration(real_data)\n","\n","                # Append things for logging\n","                d_fake_np, d_real_np, gp_np = d_fake_err.cpu().numpy(), \\\n","                        d_real_err.cpu().numpy(), gradient_penalty.cpu().numpy()\n","                grad_penalties.append(gp_np)\n","                d_real_losses.append(d_real_np)\n","                d_fake_losses.append(d_fake_np)\n","                \n","                D_losses.append(d_fake_np - d_real_np + gp_np)\n","                W_dist.append(d_real_np - d_fake_np)\n","                i=counter\n","                if counter % self.d_steps == 0:\n","                    g_err = self.gen_train_iteration()\n","                    G_losses.append((g_err.data).cpu().numpy())\n","\n","                if counter % 1000 == 999:\n","                    self.save_model(i)\n","                    self.sample(i)\n","                if counter % 100 == 99:\n","                    summary_str = 'Iteration [{}/{}] - loss_d: {}, loss_g: {}, w_dist: {}, grad_penalty: {}'\\\n","                        .format(i, total_iterations, ((d_fake_err - d_real_err + gradient_penalty)).cpu().numpy(),\n","                        (d_fake_err).cpu().numpy(), ((d_real_err - d_fake_err).data).cpu().numpy(), gp_np)\n","                    print(summary_str)\n","                    losses_f.write(summary_str)\n","                    plot_losses([G_losses, D_losses], [\"gen\", \"disc\"], self.sample_dir + \"losses.png\")\n","                    plot_losses([W_dist], [\"w_dist\"], self.sample_dir + \"dist.png\")\n","                    plot_losses([grad_penalties],[\"grad_penalties\"], self.sample_dir + \"grad.png\")\n","                    plot_losses([d_fake_losses, d_real_losses],[\"d_fake\", \"d_real\"], self.sample_dir + \"d_loss_components.png\")\n","                counter += 1\n","                if counter == total_iterations:\n","                    break\n","            np.random.shuffle(self.data)\n","\n","    def sample(self, epoch):\n","        z = to_var(torch.randn(self.batch_size, 128))\n","        self.G.eval()\n","        torch_seqs = self.G(z)\n","        seqs = (torch_seqs.data).cpu().numpy()\n","        decoded_seqs = [decode_one_seq(seq, self.inv_charmap)+\"\\n\" for seq in seqs]\n","        with open(self.sample_dir + \"sampled_{}.txt\".format(epoch), 'w+') as f:\n","            f.writelines(decoded_seqs)\n","        self.G.train()\n","\n","def main():\n","    parser = argparse.ArgumentParser(description='WGAN-GP for producing gene sequences.')\n","    parser.add_argument(\"--run_name\", default= \"realProt_50aa\", help=\"Name for output files (checkpoint and sample dir)\")\n","    parser.add_argument(\"--load_dir\", default=\"\", help=\"Option to load checkpoint from other model (Defaults to run name)\")\n","    args = parser.parse_args(args=[])\n","    model = WGAN_LangGP(run_name=args.run_name)\n","    model.train_model(args.load_dir)\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTc979m2y5Jg","outputId":"05b1cd5b-23c5-48b9-9287-9768609efca3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading dataset...\n","('C', 'T', 'T', 'T', 'A', 'T', 'T', 'G', 'A', 'A', 'A', 'T', 'A', 'T', 'T', 'T', 'T', 'T', 'C', 'C', 'T', 'C', 'A', 'C', 'T', 'G', 'T', 'A', 'G', 'T', 'C', 'G', 'A', 'T', 'G', 'A', 'A', 'A', 'A', 'C', 'A', 'G', 'C', 'G', 'C', 'G', 'C', 'A', 'A', 'A', 'A', 'G', 'T', 'G', 'C', 'A', 'G', 'C', 'T', 'G', 'A', 'T', 'G', 'T', 'T', 'G', 'A', 'C', 'G', 'C', 'A', 'A', 'T', 'C', 'G', 'T', 'G', 'G', 'G', 'A', 'A', 'A', 'A', 'G', 'T', 'T', 'C', 'G', 'C', 'G', 'G', 'C', 'A', 'C', 'A', 'A', 'C', 'C', 'C', 'T', 'G', 'A', 'C', 'A', 'A', 'C', 'C', 'C', 'C', 'G', 'A', 'C', 'C', 'G', 'A', 'A', 'A', 'A', 'C', 'G', 'A', 'C', 'A', 'A', 'C', 'C', 'C', 'T', 'T', 'G', 'G', 'C', 'C', 'T', 'C', 'G', 'T', 'G', 'G', 'C', 'C', 'G', 'C', 'A', 'A', 'C', 'G', 'C', 'A', 'A', 'G', 'G', 'A', 'G', 'C', 'C', 'A', 'C', 'A', 'C', 'A', 'G', 'C', 'T', 'G', 'G', 'T', 'C', 'A', 'A', 'G', 'A', 'T', 'G', 'A', 'C', 'T', 'T', 'C', 'G', 'A', 'T', 'C', 'C', 'C', 'G', 'A', 'T', 'C', 'C', 'T', 'G', 'G', 'T', 'T', 'C', 'C', 'G', 'T', 'G', 'A', 'T', 'C', 'C', 'C', 'A', 'G', 'C', 'T', 'G', 'C', 'C', 'A', 'G', 'C', 'C', 'G', 'T', 'A', 'C', 'C', 'C', 'G', 'T', 'A', 'C', 'A', 'G', 'T', 'C', 'G', 'C', 'G', 'T', 'C', 'T', 'T', 'G', 'T', 'T', 'T', 'A', 'G', 'C', 'A', 'A', 'A', 'C', 'T')\n","loaded 401984 lines in dataset\n","Generator_lang(\n","  (fc1): Linear(in_features=128, out_features=127488, bias=True)\n","  (block): Sequential(\n","    (0): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (1): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (2): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (3): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (4): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","  )\n","  (conv1): Conv1d(512, 5, kernel_size=(1,), stride=(1,))\n",")\n","Discriminator_lang(\n","  (block): Sequential(\n","    (0): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (1): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (2): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (3): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","    (4): ResBlock(\n","      (res_block): Sequential(\n","        (0): ReLU(inplace=True)\n","        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","        (2): ReLU(inplace=True)\n","        (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n","      )\n","    )\n","  )\n","  (conv1d): Conv1d(5, 512, kernel_size=(1,), stride=(1,))\n","  (linear): Linear(in_features=127488, out_features=1, bias=True)\n",")\n","[*] Checkpoint not found! Starting from scratch.\n","In epoch 0,n_batches is 1570,but total_iterations is4000\n","Iteration [99/4000] - loss_d: -5.497799873352051, loss_g: 9.706804275512695, w_dist: 7.467031478881836, grad_penalty: 1.9692316055297852\n","Iteration [199/4000] - loss_d: -6.265163421630859, loss_g: 11.25806713104248, w_dist: 7.204594612121582, grad_penalty: 0.9394313097000122\n","Iteration [299/4000] - loss_d: -6.789870262145996, loss_g: 0.9809404611587524, w_dist: 7.146643161773682, grad_penalty: 0.3567730784416199\n","Iteration [399/4000] - loss_d: -7.598484039306641, loss_g: 3.2911384105682373, w_dist: 8.965800285339355, grad_penalty: 1.3673162460327148\n","Iteration [499/4000] - loss_d: -7.750770568847656, loss_g: 0.5199705362319946, w_dist: 10.092663764953613, grad_penalty: 2.341893196105957\n","Iteration [599/4000] - loss_d: -7.959602355957031, loss_g: -3.1117708683013916, w_dist: 10.560932159423828, grad_penalty: 2.601330041885376\n","Iteration [699/4000] - loss_d: -8.078442573547363, loss_g: -2.0618975162506104, w_dist: 9.590587615966797, grad_penalty: 1.5121448040008545\n","Iteration [799/4000] - loss_d: -8.069866180419922, loss_g: -1.446523666381836, w_dist: 10.254605293273926, grad_penalty: 2.184738874435425\n","Iteration [899/4000] - loss_d: -8.150041580200195, loss_g: -1.0526423454284668, w_dist: 10.211801528930664, grad_penalty: 2.0617599487304688\n","Iteration [999/4000] - loss_d: -8.314752578735352, loss_g: 0.23592641949653625, w_dist: 10.28169059753418, grad_penalty: 1.9669383764266968\n","Iteration [1099/4000] - loss_d: -8.074372291564941, loss_g: -0.8565807342529297, w_dist: 11.099279403686523, grad_penalty: 3.024906873703003\n","Iteration [1199/4000] - loss_d: -8.582796096801758, loss_g: 0.166150763630867, w_dist: 10.53679084777832, grad_penalty: 1.9539949893951416\n","Iteration [1299/4000] - loss_d: -8.827409744262695, loss_g: 1.3543686866760254, w_dist: 10.809244155883789, grad_penalty: 1.9818339347839355\n","Iteration [1399/4000] - loss_d: -9.072744369506836, loss_g: 1.5676822662353516, w_dist: 11.298398971557617, grad_penalty: 2.225654363632202\n","Iteration [1499/4000] - loss_d: -8.586008071899414, loss_g: 4.779665470123291, w_dist: 11.219598770141602, grad_penalty: 2.6335902214050293\n","In epoch 1,n_batches is 1570,but total_iterations is4000\n","Iteration [1599/4000] - loss_d: -8.908924102783203, loss_g: 3.5497493743896484, w_dist: 11.178825378417969, grad_penalty: 2.2699007987976074\n","Iteration [1699/4000] - loss_d: -9.231813430786133, loss_g: 5.169276714324951, w_dist: 11.382242202758789, grad_penalty: 2.1504292488098145\n","Iteration [1799/4000] - loss_d: -9.133636474609375, loss_g: 6.659308433532715, w_dist: 10.98796558380127, grad_penalty: 1.8543291091918945\n","Iteration [1899/4000] - loss_d: -9.038516998291016, loss_g: 6.25912618637085, w_dist: 11.030458450317383, grad_penalty: 1.9919419288635254\n","Iteration [1999/4000] - loss_d: -9.06165885925293, loss_g: 9.528432846069336, w_dist: 12.328813552856445, grad_penalty: 3.2671542167663574\n","Iteration [2099/4000] - loss_d: -9.505155563354492, loss_g: 8.631473541259766, w_dist: 11.216621398925781, grad_penalty: 1.7114653587341309\n","Iteration [2199/4000] - loss_d: -9.230572700500488, loss_g: 8.422115325927734, w_dist: 10.790792465209961, grad_penalty: 1.5602195262908936\n","Iteration [2299/4000] - loss_d: -9.045476913452148, loss_g: 10.711496353149414, w_dist: 12.388553619384766, grad_penalty: 3.343076229095459\n","Iteration [2399/4000] - loss_d: -9.038848876953125, loss_g: 8.629764556884766, w_dist: 10.07525634765625, grad_penalty: 1.036407470703125\n","Iteration [2499/4000] - loss_d: -8.758810043334961, loss_g: 13.607772827148438, w_dist: 11.967430114746094, grad_penalty: 3.208620071411133\n","Iteration [2599/4000] - loss_d: -9.14661693572998, loss_g: 10.624610900878906, w_dist: 11.183126449584961, grad_penalty: 2.0365095138549805\n","Iteration [2699/4000] - loss_d: -9.247069358825684, loss_g: 10.582454681396484, w_dist: 11.09444808959961, grad_penalty: 1.8473789691925049\n","Iteration [2799/4000] - loss_d: -9.226709365844727, loss_g: 13.735523223876953, w_dist: 11.814769744873047, grad_penalty: 2.5880608558654785\n","Iteration [2899/4000] - loss_d: -9.023445129394531, loss_g: 12.646017074584961, w_dist: 10.695802688598633, grad_penalty: 1.672357439994812\n","Iteration [2999/4000] - loss_d: -8.580123901367188, loss_g: 13.60314655303955, w_dist: 12.289507865905762, grad_penalty: 3.709383964538574\n","Iteration [3099/4000] - loss_d: -8.940080642700195, loss_g: 13.402591705322266, w_dist: 11.448554992675781, grad_penalty: 2.5084738731384277\n","In epoch 2,n_batches is 1570,but total_iterations is4000\n","Iteration [3199/4000] - loss_d: -9.265335083007812, loss_g: 15.865947723388672, w_dist: 11.938358306884766, grad_penalty: 2.673023223876953\n","Iteration [3299/4000] - loss_d: -9.213665008544922, loss_g: 14.534467697143555, w_dist: 11.626474380493164, grad_penalty: 2.412808895111084\n","Iteration [3399/4000] - loss_d: -9.255561828613281, loss_g: 14.739051818847656, w_dist: 10.763408660888672, grad_penalty: 1.5078473091125488\n","Iteration [3499/4000] - loss_d: -9.096078872680664, loss_g: 14.039266586303711, w_dist: 10.206918716430664, grad_penalty: 1.1108403205871582\n","Iteration [3599/4000] - loss_d: -9.184853553771973, loss_g: 15.940746307373047, w_dist: 11.41468620300293, grad_penalty: 2.229832887649536\n","Iteration [3699/4000] - loss_d: -8.899993896484375, loss_g: 17.95342254638672, w_dist: 11.874425888061523, grad_penalty: 2.9744315147399902\n","Iteration [3799/4000] - loss_d: -9.18392562866211, loss_g: 17.51820182800293, w_dist: 12.496355056762695, grad_penalty: 3.3124289512634277\n","Iteration [3899/4000] - loss_d: -8.779528617858887, loss_g: 16.575475692749023, w_dist: 12.16612434387207, grad_penalty: 3.3865954875946045\n"]}]}]}